{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdubRUN_zhrc"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, confusion_matrix, ConfusionMatrixDisplay,\n",
        "    mean_squared_error, mean_absolute_error, precision_score,\n",
        "    recall_score, f1_score\n",
        ")\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"Student Depression Dataset.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Dataset Overview\n",
        "print(\"Dataset Overview\")\n",
        "print(data.info())\n",
        "print(\"\\nFirst 5 Rows of the Data:\")\n",
        "print(data.head())\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(data.describe())\n",
        "\n",
        "# Data Preprocessing\n",
        "# Fill missing values\n",
        "numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].mean())\n",
        "\n",
        "categorical_columns = data.select_dtypes(include=['object']).columns\n",
        "data[categorical_columns] = data[categorical_columns].fillna(data[categorical_columns].mode().iloc[0])\n",
        "\n",
        "# Encode categorical columns\n",
        "label_encoder = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    data[col] = label_encoder.fit_transform(data[col])\n",
        "\n",
        "# Data Visualization\n",
        "# Depression Distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x=\"Depression\", data=data, hue='Depression', palette='coolwarm')\n",
        "plt.title(\"Depression Distribution\", fontsize=16)\n",
        "plt.xlabel(\"Depression (0: No, 1: Yes)\", fontsize=12)\n",
        "plt.ylabel(\"Count\", fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "# Correlation Heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(data.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
        "plt.title(\"Correlation Heatmap\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "# Feature Selection\n",
        "data.drop(['id', 'City', 'Profession'], axis=1, inplace=True)\n",
        "\n",
        "# Prepare data for modeling\n",
        "X = data.drop(columns=\"Depression\")\n",
        "y = data[\"Depression\"]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Model Training and Evaluation\n",
        "models = {\n",
        "    \"Support Vector Machines\": SVC(probability=True, kernel='rbf', random_state=42),\n",
        "    \"K-Nearest Neighbors\": KNeighborsClassifier(weights='distance', metric='euclidean'),\n",
        "    \"Logistic Regression\": LogisticRegression(random_state=42, solver='liblinear'),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name}\")\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluation Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='binary')\n",
        "    recall = recall_score(y_test, y_pred, average='binary')\n",
        "    f1 = f1_score(y_test, y_pred, average='binary')\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "    # Error metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    disp = ConfusionMatrixDisplay.from_estimator(\n",
        "        model, X_test, y_test, cmap='Blues'\n",
        "    )\n",
        "    disp.ax_.set_title(f\"Confusion Matrix: {name}\")\n",
        "    plt.show()\n",
        "\n",
        "# Unsupervised Learning - KMeans Clustering\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Elbow Method to find optimal number of clusters\n",
        "inertia = []\n",
        "range_n_clusters = list(range(2, 11))\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plotting the Elbow Method\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range_n_clusters, inertia, marker='o', linestyle='-', color='b')\n",
        "plt.title(\"Elbow Method: Optimal Number of Clusters\")\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.xticks(range_n_clusters)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# KMeans Clustering with optimal k\n",
        "optimal_k = 3  # Determined from elbow plot\n",
        "kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "cluster_labels_optimal = kmeans_optimal.fit_predict(X_scaled)\n",
        "\n",
        "# Visualize clusters using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Create DataFrame with PCA components and cluster labels\n",
        "X_pca_df = pd.DataFrame(X_pca, columns=['PCA1', 'PCA2'])\n",
        "X_pca_df['Cluster'] = cluster_labels_optimal\n",
        "\n",
        "# Plot the clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(\n",
        "    x=X_pca_df['PCA1'], y=X_pca_df['PCA2'],\n",
        "    hue=X_pca_df['Cluster'], palette=\"Set1\",\n",
        "    s=100, edgecolor='black'\n",
        ")\n",
        "plt.title(f\"KMeans Clustering with {optimal_k} Clusters (PCA)\", fontsize=16)\n",
        "plt.xlabel('PCA Component 1', fontsize=12)\n",
        "plt.ylabel('PCA Component 2', fontsize=12)\n",
        "plt.show()"
      ]
    }
  ]
}